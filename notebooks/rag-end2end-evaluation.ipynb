{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:29:34.032264Z",
     "start_time": "2024-09-16T14:29:31.817744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "from kaggle_competition_assistant import KaggleCompetitionAssistant\n",
    "from kaggle_competition_assistant import llm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set the maximum number of rows to display\n",
    "pd.set_option('display.max_rows', 100)\n",
    "tqdm.pandas()\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ],
   "id": "9863a5a41e51409b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ingestion",
   "id": "c0a0b2e4649f5e48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:29:34.036894Z",
     "start_time": "2024-09-16T14:29:34.034991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_assistant(competition_slug):\n",
    "    competition_data_path = '../data/' + competition_slug\n",
    "    assistant = KaggleCompetitionAssistant(competition_slug, competition_data_path=competition_data_path, index_type='opensearch',\n",
    "                                           index_configs={'index_name': 'kaggle-assistant-' + competition_slug})\n",
    "    return assistant"
   ],
   "id": "22a6399c5cb8829c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:29:48.768613Z",
     "start_time": "2024-09-16T14:29:34.117723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "competition_slug = 'llm-zoomcamp-2024-competition'\n",
    "assistant = create_assistant(competition_slug)"
   ],
   "id": "8089f25c23b49d90",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vladkha/Desktop/Workspace/Coding/kaggle-competition-copilot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Creating document embeddings:   0%|          | 0/91 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3667a7a66db4b8786a35c7e10f53920"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## End2End evaluation",
   "id": "a1ed57dfd43a6e00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load ground truth",
   "id": "c586a4f4748b5ba3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:29:48.819169Z",
     "start_time": "2024-09-16T14:29:48.787920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_question = pd.read_csv(f'../data/evaluation/{competition_slug}-ground-truth.csv')\n",
    "ground_truth = df_question.to_dict(orient='records')\n",
    "\n",
    "df_question.sample(5)"
   ],
   "id": "987c713ae04eda5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    doc_id                                           question  \\\n",
       "12       7  What are some examples of problem IDs and thei...   \n",
       "7        3     What is the main objective of the competition?   \n",
       "29      20        How many files are provided in the dataset?   \n",
       "44      90  How impactful can prompt engineering be in thi...   \n",
       "20      13  What is the name of the column containing the ...   \n",
       "\n",
       "                                               answer  \n",
       "12  Examples include 11919 with answer 11, 8513 wi...  \n",
       "7   To predict the correct answer for each problem...  \n",
       "29                                                  7  \n",
       "44  Significant improvements in scores have been o...  \n",
       "20                                             answer  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>What are some examples of problem IDs and thei...</td>\n",
       "      <td>Examples include 11919 with answer 11, 8513 wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the main objective of the competition?</td>\n",
       "      <td>To predict the correct answer for each problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20</td>\n",
       "      <td>How many files are provided in the dataset?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>90</td>\n",
       "      <td>How impactful can prompt engineering be in thi...</td>\n",
       "      <td>Significant improvements in scores have been o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13</td>\n",
       "      <td>What is the name of the column containing the ...</td>\n",
       "      <td>answer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate assistant answers",
   "id": "91c3ea82a55d9585"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:29:48.855220Z",
     "start_time": "2024-09-16T14:29:48.849117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_assistant_answers(assistant, ground_truth, retrieval_configs={}, num_results=5, generation_llm_prompt=None):\n",
    "    answers = {}\n",
    "    \n",
    "    for i, rec in enumerate(tqdm(ground_truth)):\n",
    "        if i in answers:\n",
    "            continue\n",
    "    \n",
    "        answer_assistant = assistant.query(rec['question'],\n",
    "                                           retrieval_configs=retrieval_configs, retrieval_n_results=num_results,\n",
    "                                           generation_llm_prompt=generation_llm_prompt)[0]\n",
    "    \n",
    "        answers[i] = {\n",
    "            'question': rec['question'],\n",
    "            'answer_assistant': answer_assistant,\n",
    "            'answer_orig': rec['answer'],\n",
    "            'document_orig': rec['doc_id'],\n",
    "        }\n",
    "        \n",
    "    return answers"
   ],
   "id": "986175e87841fba0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:32:48.084028Z",
     "start_time": "2024-09-16T14:29:48.886646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assistant_answers = generate_assistant_answers(assistant, ground_truth,\n",
    "                                               retrieval_configs={\n",
    "                                                   'search_type': 'hybrid_rff',\n",
    "                                                   'boost_dict': {\"source\": 9.15, \"section\": 2.21, \"text\": 0.63} # after hyperparameter tuning\n",
    "                                               }, num_results=10)"
   ],
   "id": "cb6076b441d7f6fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "890fbbc05eae4f299794fd61060cedd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:32:48.142960Z",
     "start_time": "2024-09-16T14:32:48.116897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_answers = pd.DataFrame(assistant_answers.values(), columns=['question', 'answer_assistant', 'answer_orig', 'document_orig'])\n",
    "df_answers.to_csv(f'../data/evaluation/{competition_slug}-rag-answers.csv', index=False)\n",
    "df_answers.sample(5, random_state=42)"
   ],
   "id": "b150fd226ae2d116",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             question  \\\n",
       "39          What is the score of ArturG's submission?   \n",
       "25  What is the unique identifier for each problem...   \n",
       "26  What is the language of the mathematical probl...   \n",
       "43  What accuracy did the Claude Sonnet 3.5 model ...   \n",
       "35       How many columns are present in the dataset?   \n",
       "\n",
       "                                       answer_copilot  \\\n",
       "39    ArturG's submission has a score of **0.93750**.   \n",
       "25  The unique identifier for each problem in the ...   \n",
       "26  The mathematical problem statements in the `ru...   \n",
       "43  The Claude Sonnet 3.5 model achieved an accura...   \n",
       "35                        The dataset has 19 columns.   \n",
       "\n",
       "                                          answer_orig  document_orig  \n",
       "39                                            0.93750             29  \n",
       "25                                         problem_id             17  \n",
       "26                                            Russian             18  \n",
       "43  The Claude Sonnet 3.5 model achieved 85% accur...             89  \n",
       "35                                                 19             25  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_copilot</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document_orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>What is the score of ArturG's submission?</td>\n",
       "      <td>ArturG's submission has a score of **0.93750**.</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What is the unique identifier for each problem...</td>\n",
       "      <td>The unique identifier for each problem in the ...</td>\n",
       "      <td>problem_id</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What is the language of the mathematical probl...</td>\n",
       "      <td>The mathematical problem statements in the `ru...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>What accuracy did the Claude Sonnet 3.5 model ...</td>\n",
       "      <td>The Claude Sonnet 3.5 model achieved an accura...</td>\n",
       "      <td>The Claude Sonnet 3.5 model achieved 85% accur...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>How many columns are present in the dataset?</td>\n",
       "      <td>The dataset has 19 columns.</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cosine similarity metric",
   "id": "f47e221c46b16b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:32:58.344103Z",
     "start_time": "2024-09-16T14:32:48.168815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'Lajavaness/bilingual-embedding-large' # best Semantic Textual Similarity model according to https://huggingface.co/spaces/mteb/leaderboard \n",
    "model = SentenceTransformer(model_name, trust_remote_code=True)"
   ],
   "id": "ad09b8aa486c9a6c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:32:58.393169Z",
     "start_time": "2024-09-16T14:32:58.390935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_rag = record['answer_assistant']\n",
    "    \n",
    "    v_rag = model.encode(answer_rag)\n",
    "    v_orig = model.encode(answer_orig)\n",
    "    \n",
    "    return v_rag.dot(v_orig)"
   ],
   "id": "180cd3368836cfc2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:33:10.423643Z",
     "start_time": "2024-09-16T14:32:58.425542Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers['cosine_similarity'] = df_answers.progress_apply(lambda row: compute_similarity(row), axis=1)",
   "id": "f55825a315ab8c38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54972372e10f426687484aa9737a60c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:33:10.436746Z",
     "start_time": "2024-09-16T14:33:10.431408Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers.head()",
   "id": "6e0449cb74d812b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            question  \\\n",
       "0        What is the name of the Kaggle competition?   \n",
       "1  What type of problems are participants asked t...   \n",
       "2     What is the main objective of the competition?   \n",
       "3  Where do the mathematics problems originate from?   \n",
       "4                  How were the problems translated?   \n",
       "\n",
       "                                      answer_copilot  \\\n",
       "0  The Kaggle competition is called \"LLM Zoomcamp...   \n",
       "1  Participants in the LLM Zoomcamp 2024 Competit...   \n",
       "2  The main objective of the competition is to de...   \n",
       "3  The mathematics problems originate from the [Е...   \n",
       "4  The problems were translated from Russian to E...   \n",
       "\n",
       "                                         answer_orig  document_orig  \\\n",
       "0                      LLM Zoomcamp 2024 Competition              0   \n",
       "1                  High school mathematical problems              1   \n",
       "2  Participants need to solve high school mathema...              2   \n",
       "3  The problems originate from the Unified State ...              2   \n",
       "4  Each problem was translated to English using G...              2   \n",
       "\n",
       "   cosine_similarity  \n",
       "0           0.811396  \n",
       "1           0.574622  \n",
       "2           0.639532  \n",
       "3           0.965727  \n",
       "4           0.892885  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_copilot</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document_orig</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the name of the Kaggle competition?</td>\n",
       "      <td>The Kaggle competition is called \"LLM Zoomcamp...</td>\n",
       "      <td>LLM Zoomcamp 2024 Competition</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What type of problems are participants asked t...</td>\n",
       "      <td>Participants in the LLM Zoomcamp 2024 Competit...</td>\n",
       "      <td>High school mathematical problems</td>\n",
       "      <td>1</td>\n",
       "      <td>0.574622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the main objective of the competition?</td>\n",
       "      <td>The main objective of the competition is to de...</td>\n",
       "      <td>Participants need to solve high school mathema...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.639532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where do the mathematics problems originate from?</td>\n",
       "      <td>The mathematics problems originate from the [Е...</td>\n",
       "      <td>The problems originate from the Unified State ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How were the problems translated?</td>\n",
       "      <td>The problems were translated from Russian to E...</td>\n",
       "      <td>Each problem was translated to English using G...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.892885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:33:10.481274Z",
     "start_time": "2024-09-16T14:33:10.474843Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers['cosine_similarity'].describe()",
   "id": "433d988f38d41968",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    45.000000\n",
       "mean      0.660743\n",
       "std       0.213638\n",
       "min       0.006070\n",
       "25%       0.464731\n",
       "50%       0.679022\n",
       "75%       0.848746\n",
       "max       1.000000\n",
       "Name: cosine_similarity, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Distribution of cosine similarities plot",
   "id": "592ea9c3c8543164"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:33:10.885652Z",
     "start_time": "2024-09-16T14:33:10.512748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sns.histplot(df_answers['cosine_similarity'], kde=True, stat='density')\n",
    "\n",
    "plt.title(\"RAG End2End evaluation\")\n",
    "plt.xlabel(\"Generated answer vs Ground truth answer Cosine Similarity\")\n",
    "plt.legend()"
   ],
   "id": "bc24f848a1f0d879",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pm/jl5fx4c52mg00plwqrbg03tm0000gn/T/ipykernel_2990/3266868292.py:5: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x15ba306d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LLM-as-a-Judge",
   "id": "fc6982932f03cf3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:33:10.938208Z",
     "start_time": "2024-09-16T14:33:10.934693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_judge_aa_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTIALLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Question: {question}\n",
    "Generated Answer: {answer_assistant}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using any code blocks:\n",
    "{{\n",
    "  \"relevance\": \"NON_RELEVANT\" | \"PARTIALLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "llm_judge_aq_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer to a given question.\n",
    "Based on the relevance of the generated answer, you will classify it as \"NON_RELEVANT\", \"PARTIALLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "The context of the question and answer relate to a Kaggle competition.\n",
    "Avoid:\n",
    "- trying to determine the correctness of the answer. Just focus on generic relevance of answer to the topic of the question.\n",
    "- trying to reason about the specificity of the answer or additional context. Just focus on generic relevance of answer to the topic of the question.\n",
    "\n",
    "Here is the data for evaluation:\n",
    "Question: {question}\n",
    "Generated answer: {answer_assistant}\n",
    "\n",
    "Please analyze the content of the generated answer in relation to the question and provide your evaluation in parsable JSON without using any code blocks:\n",
    "{{\n",
    "  \"relevance\": \"NON_RELEVANT\" | \"PARTIALLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ],
   "id": "6e1a940f492042e3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sample prompt",
   "id": "bac28a1d50d58545"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:33:10.990598Z",
     "start_time": "2024-09-16T14:33:10.988333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q = df_answers['question'].values[0]\n",
    "a_c = df_answers['answer_assistant'].values[0]\n",
    "a_o = df_answers['answer_orig'].values[0]\n",
    "print(llm_judge_aa_template.format(question=q, answer_assistant=a_c, answer_orig=a_o))"
   ],
   "id": "f4858dd7874a7170",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
      "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
      "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
      "it as \"NON_RELEVANT\", \"PARTIALLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Original Answer: LLM Zoomcamp 2024 Competition\n",
      "Question: What is the name of the Kaggle competition?\n",
      "Generated Answer: The Kaggle competition is called \"LLM Zoomcamp 2024 Competition\".\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the original\n",
      "answer and provide your evaluation in parsable JSON without using any code blocks:\n",
      "{\n",
      "  \"relevance\": \"NON_RELEVANT\" | \"PARTIALLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:33:11.033150Z",
     "start_time": "2024-09-16T14:33:11.029046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def llm_as_a_judge(prompt_template, records):\n",
    "    evaluations = []\n",
    "    for i,record in tqdm(records.iterrows(), desc='Evaluating with LLM as a judge', total=len(records)):\n",
    "        prompt = prompt_template.format(**record)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                evaluation = llm(prompt)[0]\n",
    "                \n",
    "                # still some scrubbing of code blocks required\n",
    "                evaluation = evaluation.removeprefix(\"```json\").removesuffix(\"```\")\n",
    "                \n",
    "                evaluation = json.loads(evaluation)\n",
    "                evaluations.append(evaluation)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(evaluation)\n",
    "                print(f'Failed generating judgement: {e}. Trying again...')\n",
    "        \n",
    "    df_evaluations = pd.DataFrame(evaluations)\n",
    "    \n",
    "    return df_evaluations"
   ],
   "id": "33d469e8c4e77de9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:35:04.289709Z",
     "start_time": "2024-09-16T14:33:11.061708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_evaluations_aa = llm_as_a_judge(llm_judge_aa_template, df_answers)\n",
    "df_answers['aa_relevance'] = df_evaluations_aa['relevance']\n",
    "df_answers['aa_relevance_explanation'] = df_evaluations_aa['explanation']"
   ],
   "id": "77c49d357ad9e506",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluating with LLM as a judge:   0%|          | 0/45 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33ad3e8091c74b4d9064f54d0a21f2e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:35:04.322153Z",
     "start_time": "2024-09-16T14:35:04.313948Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers['aa_relevance'].value_counts()",
   "id": "2b45c7594008f059",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa_relevance\n",
       "RELEVANT              37\n",
       "PARTIALLY_RELEVANT     4\n",
       "NON_RELEVANT           4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:35:04.365871Z",
     "start_time": "2024-09-16T14:35:04.362134Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers['aa_relevance'].value_counts(normalize=True)",
   "id": "92cb6dc15aa7ee9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa_relevance\n",
       "RELEVANT              0.822222\n",
       "PARTIALLY_RELEVANT    0.088889\n",
       "NON_RELEVANT          0.088889\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:35:04.380118Z",
     "start_time": "2024-09-16T14:35:04.373928Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers[df_answers['aa_relevance'] == 'NON_RELEVANT']",
   "id": "cdd0ca9946a3e50a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             question  \\\n",
       "12  What are some examples of problem IDs and thei...   \n",
       "28  What is the initial value for the 'answer' col...   \n",
       "36                    Who won the Kaggle competition?   \n",
       "41  What are some suggested ways to utilize large ...   \n",
       "\n",
       "                                       answer_copilot  \\\n",
       "12  The provided context doesn't include specific ...   \n",
       "28  The initial value for the 'answer' column in t...   \n",
       "36  The provided context does not mention the winn...   \n",
       "41  The provided context does not suggest specific...   \n",
       "\n",
       "                                          answer_orig  document_orig  \\\n",
       "12  Examples include 11919 with answer 11, 8513 wi...              7   \n",
       "28                                          No answer             19   \n",
       "36                                    CHINONSO ODIAKA             26   \n",
       "41  Experimenting with various system prompts, suc...             87   \n",
       "\n",
       "    cosine_similarity  aa_relevance  \\\n",
       "12           0.408701  NON_RELEVANT   \n",
       "28           0.600175  NON_RELEVANT   \n",
       "36           0.006070  NON_RELEVANT   \n",
       "41           0.502092  NON_RELEVANT   \n",
       "\n",
       "                             aa_relevance_explanation  \n",
       "12  The generated answer completely misses the exa...  \n",
       "28  The original answer is 'No answer', meaning th...  \n",
       "36  The generated answer is not relevant to the or...  \n",
       "41  The generated answer completely ignores the in...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_copilot</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document_orig</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>aa_relevance</th>\n",
       "      <th>aa_relevance_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are some examples of problem IDs and thei...</td>\n",
       "      <td>The provided context doesn't include specific ...</td>\n",
       "      <td>Examples include 11919 with answer 11, 8513 wi...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer completely misses the exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the initial value for the 'answer' col...</td>\n",
       "      <td>The initial value for the 'answer' column in t...</td>\n",
       "      <td>No answer</td>\n",
       "      <td>19</td>\n",
       "      <td>0.600175</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The original answer is 'No answer', meaning th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Who won the Kaggle competition?</td>\n",
       "      <td>The provided context does not mention the winn...</td>\n",
       "      <td>CHINONSO ODIAKA</td>\n",
       "      <td>26</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer is not relevant to the or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>What are some suggested ways to utilize large ...</td>\n",
       "      <td>The provided context does not suggest specific...</td>\n",
       "      <td>Experimenting with various system prompts, suc...</td>\n",
       "      <td>87</td>\n",
       "      <td>0.502092</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer completely ignores the in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:38:54.281836Z",
     "start_time": "2024-09-16T14:35:04.404012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_evaluations_aq = llm_as_a_judge(llm_judge_aq_template, df_answers)\n",
    "df_answers['aq_relevance'] = df_evaluations_aq['relevance']\n",
    "df_answers['aq_relevance_explanation'] = df_evaluations_aq['explanation']"
   ],
   "id": "86ef2cb9ff7ff9ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluating with LLM as a judge:   0%|          | 0/45 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6385fd27f324be4afb170a9fc648358"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:38:54.355730Z",
     "start_time": "2024-09-16T14:38:54.352552Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers['aq_relevance'].value_counts()",
   "id": "1e8dd21d2bf4908f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aq_relevance\n",
       "RELEVANT              39\n",
       "PARTIALLY_RELEVANT     4\n",
       "NON_RELEVANT           2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:38:54.531588Z",
     "start_time": "2024-09-16T14:38:54.526804Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers['aq_relevance'].value_counts(normalize=True)",
   "id": "cea0029ac4440908",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aq_relevance\n",
       "RELEVANT              0.866667\n",
       "PARTIALLY_RELEVANT    0.088889\n",
       "NON_RELEVANT          0.044444\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:38:54.743378Z",
     "start_time": "2024-09-16T14:38:54.737585Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers[df_answers['aq_relevance'] == 'PARTIALLY_RELEVANT']",
   "id": "5e0b8b0c316c7184",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             question  \\\n",
       "3   Where do the mathematics problems originate from?   \n",
       "8     How is accuracy calculated in this competition?   \n",
       "10  What is the primary evaluation metric for this...   \n",
       "14  What is the recommended action to take before ...   \n",
       "\n",
       "                                       answer_copilot  \\\n",
       "3   The mathematics problems originate from the [Е...   \n",
       "8   Accuracy is calculated by dividing the number ...   \n",
       "10  The primary evaluation metric for this competi...   \n",
       "14  Before submitting your file, ensure you valida...   \n",
       "\n",
       "                                          answer_orig  document_orig  \\\n",
       "3   The problems originate from the Unified State ...              2   \n",
       "8   Accuracy is defined as the proportion of corre...              4   \n",
       "10                                           Accuracy              6   \n",
       "14  Make sure to validate the submission file form...              7   \n",
       "\n",
       "    cosine_similarity aa_relevance  \\\n",
       "3            0.965727     RELEVANT   \n",
       "8            0.900809     RELEVANT   \n",
       "10           0.464731     RELEVANT   \n",
       "14           0.952906     RELEVANT   \n",
       "\n",
       "                             aa_relevance_explanation        aq_relevance  \\\n",
       "3   The generated answer is identical to the origi...  PARTIALLY_RELEVANT   \n",
       "8   Both the original and generated answers define...  PARTIALLY_RELEVANT   \n",
       "10  The generated answer correctly identifies 'acc...  PARTIALLY_RELEVANT   \n",
       "14  The generated answer is essentially a rephrase...  PARTIALLY_RELEVANT   \n",
       "\n",
       "                             aq_relevance_explanation  \n",
       "3   The answer mentions a source of math problems,...  \n",
       "8   The answer provides a general definition of ac...  \n",
       "10  While the answer mentions an evaluation metric...  \n",
       "14  The answer is partially relevant as it mention...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_copilot</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document_orig</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>aa_relevance</th>\n",
       "      <th>aa_relevance_explanation</th>\n",
       "      <th>aq_relevance</th>\n",
       "      <th>aq_relevance_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where do the mathematics problems originate from?</td>\n",
       "      <td>The mathematics problems originate from the [Е...</td>\n",
       "      <td>The problems originate from the Unified State ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965727</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is identical to the origi...</td>\n",
       "      <td>PARTIALLY_RELEVANT</td>\n",
       "      <td>The answer mentions a source of math problems,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How is accuracy calculated in this competition?</td>\n",
       "      <td>Accuracy is calculated by dividing the number ...</td>\n",
       "      <td>Accuracy is defined as the proportion of corre...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.900809</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>Both the original and generated answers define...</td>\n",
       "      <td>PARTIALLY_RELEVANT</td>\n",
       "      <td>The answer provides a general definition of ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the primary evaluation metric for this...</td>\n",
       "      <td>The primary evaluation metric for this competi...</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>6</td>\n",
       "      <td>0.464731</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer correctly identifies 'acc...</td>\n",
       "      <td>PARTIALLY_RELEVANT</td>\n",
       "      <td>While the answer mentions an evaluation metric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the recommended action to take before ...</td>\n",
       "      <td>Before submitting your file, ensure you valida...</td>\n",
       "      <td>Make sure to validate the submission file form...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.952906</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer is essentially a rephrase...</td>\n",
       "      <td>PARTIALLY_RELEVANT</td>\n",
       "      <td>The answer is partially relevant as it mention...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:28:05.289270Z",
     "start_time": "2024-09-16T15:28:05.283290Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers.to_csv(f'../data/evaluation/{competition_slug}-end2end-evaluation.csv', index=False)",
   "id": "c24cebbf050e9224",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation for 2nd dataset",
   "id": "b74f5540a458a345"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:49:32.139532Z",
     "start_time": "2024-09-16T14:49:32.135252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_end2end(competition_slug: str, \n",
    "                     retrieval_configs: Optional[dict] = None, num_results: int = 5, \n",
    "                     generation_llm_prompt: Optional[str] = None):\n",
    "    assistant = create_assistant(competition_slug)\n",
    "    \n",
    "    df_question = pd.read_csv(f'../data/evaluation/{competition_slug}-ground-truth.csv')\n",
    "    ground_truth = df_question.to_dict(orient='records')\n",
    "    \n",
    "    assistant_answers = generate_assistant_answers(assistant, ground_truth,\n",
    "                                                 retrieval_configs=retrieval_configs, num_results=num_results, generation_llm_prompt=generation_llm_prompt)\n",
    "    \n",
    "    df_answers = pd.DataFrame(assistant_answers.values(), columns=['question', 'answer_assistant', 'answer_orig', 'document_orig'])\n",
    "    # df_answers.to_csv(f'../data/evaluation/{competition_slug}-rag-answers.csv', index=False)\n",
    "    \n",
    "    # cosine sim\n",
    "    df_answers['cosine_similarity'] = df_answers.progress_apply(lambda row: compute_similarity(row), axis=1)\n",
    "    \n",
    "    # aa relevance\n",
    "    df_evaluations_aa = llm_as_a_judge(llm_judge_aa_template, df_answers)\n",
    "    df_answers['aa_relevance'] = df_evaluations_aa['relevance']\n",
    "    df_answers['aa_relevance_explanation'] = df_evaluations_aa['explanation']\n",
    "    \n",
    "    # aq relevance\n",
    "    df_evaluations_aq = llm_as_a_judge(llm_judge_aq_template, df_answers)\n",
    "    df_answers['aq_relevance'] = df_evaluations_aq['relevance']\n",
    "    df_answers['aq_relevance_explanation'] = df_evaluations_aq['explanation']\n",
    "    \n",
    "    return df_answers"
   ],
   "id": "4d9a51e5202c650e",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:16:04.581246Z",
     "start_time": "2024-09-16T14:49:33.740730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "competition_slug = 'rohlik-orders-forecasting-challenge'\n",
    "df_answers_2 = evaluate_end2end(competition_slug,\n",
    "                                retrieval_configs={'search_type': 'semantic'}, num_results=10)"
   ],
   "id": "aa43ccb5342a7b99",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vladkha/Desktop/Workspace/Coding/kaggle-competition-copilot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Creating document embeddings:   0%|          | 0/1117 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ca19bbdd07847709a45b946f2c2506b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc2bfe66109446a6bb2e05db9072d114"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f55cef6bbf2f40acb354e447f35ecfe7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Evaluating with LLM as a judge:   0%|          | 0/115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d08e3b28658c4cda989b181d6715d6b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Evaluating with LLM as a judge:   0%|          | 0/115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8dbc5d14149344d69e95c3dfb05c0a14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"relevance\": \"PARTIALLY_RELEVANT\",\n",
      "  \"explanation\": \"The answer acknowledges the lack of explicitly stated restrictions, but it also mentions various tools that can be used for prediction generation, which is tangentially related to the question's focus on hardware/software restrictions.\"\n",
      "}\n",
      "``` \n",
      "\n",
      "Failed generating judgement: Extra data: line 6 column 1 (char 297). Trying again...\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:16:05.720011Z",
     "start_time": "2024-09-16T15:16:05.715023Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_2['cosine_similarity'].describe()",
   "id": "b38fd114677a35d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    115.000000\n",
       "mean       0.681473\n",
       "std        0.187289\n",
       "min        0.109915\n",
       "25%        0.558338\n",
       "50%        0.722075\n",
       "75%        0.826727\n",
       "max        0.992766\n",
       "Name: cosine_similarity, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:22:02.197751Z",
     "start_time": "2024-09-16T15:22:02.193915Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_2['aa_relevance'].value_counts()",
   "id": "47986dbfd908b7f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa_relevance\n",
       "RELEVANT              61\n",
       "PARTIALLY_RELEVANT    34\n",
       "NON_RELEVANT          20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:22:05.803838Z",
     "start_time": "2024-09-16T15:22:05.798900Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_2['aa_relevance'].value_counts(normalize=True)",
   "id": "3c80e9b01c7fced6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa_relevance\n",
       "RELEVANT              0.530435\n",
       "PARTIALLY_RELEVANT    0.295652\n",
       "NON_RELEVANT          0.173913\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:16:06.136342Z",
     "start_time": "2024-09-16T15:16:06.133154Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_2['aq_relevance'].value_counts()",
   "id": "bd1a825db343b400",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aq_relevance\n",
       "RELEVANT              67\n",
       "PARTIALLY_RELEVANT    34\n",
       "NON_RELEVANT          14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:24:23.924263Z",
     "start_time": "2024-09-16T15:24:23.916727Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_2['aq_relevance'].value_counts(normalize=True)",
   "id": "be1c163a02683025",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aq_relevance\n",
       "RELEVANT              0.582609\n",
       "PARTIALLY_RELEVANT    0.295652\n",
       "NON_RELEVANT          0.121739\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:28:23.042336Z",
     "start_time": "2024-09-16T15:28:23.027469Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_2.to_csv(f'../data/evaluation/{competition_slug}-end2end-evaluation.csv', index=False)",
   "id": "cd89d55dbc5686da",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation with different prompt",
   "id": "65ac326ec8a1484b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:29:00.070384Z",
     "start_time": "2024-09-16T15:29:00.065374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "improved_prompt_template = \"\"\"\n",
    "You are a Kaggle competition assistant.\n",
    "Answer the QUESTION based on the CONTEXT from the web page about the competition.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "Be concise and to the point.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()"
   ],
   "id": "c2eff5c623da4b74",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:39:17.050378Z",
     "start_time": "2024-09-16T15:29:00.303569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "competition_slug = 'llm-zoomcamp-2024-competition'\n",
    "df_answers_zoomcamp_2 = evaluate_end2end(competition_slug,\n",
    "                                         retrieval_configs={\n",
    "                                             'search_type': 'hybrid_rff',\n",
    "                                             'boost_dict': {\"source\": 9.15, \"section\": 2.21, \"text\": 0.63}\n",
    "                                         }, num_results=10,\n",
    "                                         generation_llm_prompt=improved_prompt_template)\n",
    "df_answers_zoomcamp_2.to_csv(f'../data/evaluation/{competition_slug}-end2end-evaluation2.csv', index=False)"
   ],
   "id": "47ea1be466e51538",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vladkha/Desktop/Workspace/Coding/kaggle-competition-copilot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Creating document embeddings:   0%|          | 0/91 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96c901195a0d4ce78d91bd54ac9a17a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c62fb6c42b72403b856dbe9127f946d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afe20aca78014e77bf3d19e111ab7c76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Evaluating with LLM as a judge:   0%|          | 0/45 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0978eab71e824277913f8888da37d367"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"relevance\": \"PARTIALLY_RELEVANT\",\n",
      "  \"explanation\": \"The generated answer mentions solving high school math problems, which aligns with the original answer. However, it adds details about language and model development that are not present in the original answer. Therefore, it's partially relevant.\"\n",
      "}\n",
      "``` \n",
      "\n",
      "Failed generating judgement: Extra data: line 6 column 1 (char 308). Trying again...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating with LLM as a judge:   0%|          | 0/45 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d43ec28fda714039b87a2adb9cfa41a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:39:17.098221Z",
     "start_time": "2024-09-16T15:39:17.062668Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_zoomcamp_2['cosine_similarity'].describe()",
   "id": "fd6caaff01c13929",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    45.000000\n",
       "mean      0.830977\n",
       "std       0.215939\n",
       "min       0.035656\n",
       "25%       0.795087\n",
       "50%       0.902256\n",
       "75%       1.000000\n",
       "max       1.000000\n",
       "Name: cosine_similarity, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:39:17.133915Z",
     "start_time": "2024-09-16T15:39:17.123714Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_zoomcamp_2['aa_relevance'].value_counts()",
   "id": "d24a31080543d0f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa_relevance\n",
       "RELEVANT              35\n",
       "PARTIALLY_RELEVANT     5\n",
       "NON_RELEVANT           5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:39:17.217284Z",
     "start_time": "2024-09-16T15:39:17.205822Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_zoomcamp_2['aa_relevance'].value_counts(normalize=True)",
   "id": "4eced2550f70c828",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa_relevance\n",
       "RELEVANT              0.777778\n",
       "PARTIALLY_RELEVANT    0.111111\n",
       "NON_RELEVANT          0.111111\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:39:17.318556Z",
     "start_time": "2024-09-16T15:39:17.299511Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_zoomcamp_2['aq_relevance'].value_counts()",
   "id": "2d9f46a76f23c1f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aq_relevance\n",
       "RELEVANT              26\n",
       "PARTIALLY_RELEVANT    11\n",
       "NON_RELEVANT           8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:39:17.426258Z",
     "start_time": "2024-09-16T15:39:17.422572Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_zoomcamp_2['aq_relevance'].value_counts(normalize=True)",
   "id": "9aaa26ebe45e905e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aq_relevance\n",
       "RELEVANT              0.577778\n",
       "PARTIALLY_RELEVANT    0.244444\n",
       "NON_RELEVANT          0.177778\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:16:16.696390Z",
     "start_time": "2024-09-16T15:39:17.652936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "competition_slug = 'rohlik-orders-forecasting-challenge'\n",
    "df_answers_rohlik_2 = evaluate_end2end(competition_slug,\n",
    "                                       retrieval_configs={'search_type': 'semantic'}, num_results=10,\n",
    "                                       generation_llm_prompt=improved_prompt_template)\n",
    "df_answers_rohlik_2.to_csv(f'../data/evaluation/{competition_slug}-end2end-evaluation2.csv', index=False)"
   ],
   "id": "ce7987e5b7ab32e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vladkha/Desktop/Workspace/Coding/kaggle-competition-copilot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Creating document embeddings:   0%|          | 0/1117 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b63d59f97fe4c558c9d17ea5f5c97c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "823d519783224946a47e48b4a4244fd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57bb2cd8a2dc46db9fda6963f3521fae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Evaluating with LLM as a judge:   0%|          | 0/115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea604443665241bb8d749449d4d7e227"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Evaluating with LLM as a judge:   0%|          | 0/115 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca5019c1db0f48b1a132b7ee1c17aa65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"relevance\": \"PARTIALLY_RELEVANT\",\n",
      "  \"explanation\": \"The answer focuses on using the features to \"correct\" the target, which is not directly addressing the question of dealing with missing features in the test set. However, it does mention training separate models to predict the value of the missing features, which is partially relevant to the question.\"\n",
      "}\n",
      "\n",
      "Failed generating judgement: Expecting ',' delimiter: line 4 column 64 (char 103). Trying again...\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:16:16.902482Z",
     "start_time": "2024-09-16T16:16:16.887518Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_rohlik_2['cosine_similarity'].describe()",
   "id": "e17c138d8a25ff41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    115.000000\n",
       "mean       0.756441\n",
       "std        0.184881\n",
       "min        0.067134\n",
       "25%        0.634209\n",
       "50%        0.771948\n",
       "75%        0.905124\n",
       "max        1.000000\n",
       "Name: cosine_similarity, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:16:16.928235Z",
     "start_time": "2024-09-16T16:16:16.918007Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_rohlik_2['aa_relevance'].value_counts()",
   "id": "6a2ce6696de5559e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa_relevance\n",
       "RELEVANT              64\n",
       "PARTIALLY_RELEVANT    38\n",
       "NON_RELEVANT          13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:16:16.952418Z",
     "start_time": "2024-09-16T16:16:16.948139Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_rohlik_2['aa_relevance'].value_counts(normalize=True)",
   "id": "54977d0eaf3afe5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aa_relevance\n",
       "RELEVANT              0.556522\n",
       "PARTIALLY_RELEVANT    0.330435\n",
       "NON_RELEVANT          0.113043\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:16:17.026545Z",
     "start_time": "2024-09-16T16:16:17.023091Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_rohlik_2['aq_relevance'].value_counts()",
   "id": "1d591d2e3901b931",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aq_relevance\n",
       "RELEVANT              57\n",
       "PARTIALLY_RELEVANT    48\n",
       "NON_RELEVANT          10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T16:16:17.138581Z",
     "start_time": "2024-09-16T16:16:17.134716Z"
    }
   },
   "cell_type": "code",
   "source": "df_answers_rohlik_2['aq_relevance'].value_counts(normalize=True)",
   "id": "7d0ca3ee8dae7de0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aq_relevance\n",
       "RELEVANT              0.495652\n",
       "PARTIALLY_RELEVANT    0.417391\n",
       "NON_RELEVANT          0.086957\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "64dd4f82b92ae1f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
