[Andrew Tratz](/jademonk) · 2nd in this Competition · Posted 6 days ago


### LLM performance metric analysis
I've been pondering for a while how to analyze the performance of LLM bots in
a more objective way. Unfortunately, many factors including randomness, few
episodes, agent alpha, pairing bias, bad guessers, bad answerers, etc. inject
a lot of noise into the competition environment making it difficult to
distinguish truly "good" bots versus those which are just "lucky."
The biggest sources of variability are probably:
  1. The presence of alpha bots
  2. Pairing is not entirely random
  3. Low number of episodes means even the best LLMs have only a handful of LLM wins
The first problem is easily addressed, since we can filter out episodes with
alpha behavior to find those which are behaving like true LLMs (or other
similar strategies). But #2 and #3 are tricky.
In an attempt to address these issues, I've put together an analysis at:
<https://www.kaggle.com/code/jademonk/llm-performance-metrics>
I use phrase-BERT to calculate the best cosine similarity between guesses and
the actual keyword. For wins, this will be 1.0. Similar guesses are close to
1.0 and distant guesses are much lower. This gives some idea of "partial
credit" for failed episodes which _nearly_ got the right answer compared to
dumb answerers that picked the same word over and over, for example.
Conceptually, I want to assess how good of a _partner_ each bot is. So, if
it's a guesser, it should allow its answerer to do a better job than average.
If it is an answerer, it should help its guesser do a better job than average.
To do this, I calculate the performance "lift" that the agents provide to
their partners, by increasing the cosine similarity of their match-up above
and beyond what an average partner should provide to them.
To use a sports analogy, it's like computing how much benefit a player batting
.308 provides to a team compared to another who bats .251.
This results in three new metrics:
  * Answerer Lift: how much more likely is it to ask smart questions to help its answerer partner do better than normal
  * Guesser Lift: how much more likely is it to provide reliable answers to help its guesser get to the right keyword
  * Combined Lift: a simple addition of the two
These metrics help address problem #2 above, since if two strong bots are
paired together they must prove that they can help each other much better _as
a team_ compared to two other bots. And strong + weak can also outperform
expectations even if they don't get a win.
The "dream team" pair up, according to this analysis, would be c-number as
guesser (Lift: 0.1585) and Umut Toygar Göz as answerer (Lift: 0.0607).
This is still far from perfect. There are probably some additional
normalizations, etc. which might make it even better, and there are almost
certainly some biases still present (and we are dealing with low number of
episodes per agent, which is unavoidable). But thought I'd get this out for
comment from the community. Welcome any suggestions on this approach.
**_UPDATED to reflect data as of Aug 29 5am UTC_**
Here are the top 100 bots according to this notebook:
Rank | SubmissionId | AnswererLift | GuesserLift | CombinedLift | Team Name  
---|---|---|---|---|---  
1 | 39527069 | 0.1523 | 0.0367 | 0.1890 | majimekun  
2 | 39529992 | 0.1585 | 0.0118 | 0.1703 | c-number  
3 | 39528678 | 0.1150 | 0.0463 | 0.1614 | agent alpha  
4 | 39526145 | 0.1443 | 0.0137 | 0.1581 | majimekun  
5 | 39530133 | 0.1491 | 0.0074 | 0.1565 | c-number  
6 | 39527171 | 0.1541 | -0.004 | 0.1493 | zzzz  
7 | 39531091 | 0.1152 | 0.0326 | 0.1478 | Moro  
8 | 39526710 | 0.1240 | 0.0236 | 0.1476 | agent alpha  
9 | 39527858 | 0.1036 | 0.0403 | 0.1439 | SpiralTip  
10 | 39499327 | 0.1108 | 0.0329 | 0.1438 | Tricksy Hobbitses  
11 | 39529611 | 0.1156 | 0.0280 | 0.1436 | kambehmw  
12 | 39490735 | 0.0810 | 0.0606 | 0.1417 | Umut Toygar G??z  
13 | 39531024 | 0.1120 | 0.0283 | 0.1403 | FullEmpty  
14 | 39531321 | 0.1117 | 0.0275 | 0.1393 | Behavior Cloners  
15 | 39457390 | 0.1132 | 0.0256 | 0.1389 | Kabaev Anton  
16 | 39514797 | 0.1039 | 0.0340 | 0.1380 | Cantaloupe  
17 | 39531407 | 0.1105 | 0.0263 | 0.1368 | Joonas Maanonen  
18 | 39527133 | 0.1024 | 0.0304 | 0.1328 | Pascal Pfeiffer  
19 | 39531390 | 0.0842 | 0.0485 | 0.1328 | Alex Glinsky  
20 | 39528776 | 0.1040 | 0.0283 | 0.1324 | Tran Anh Quan  
21 | 39528529 | 0.0949 | 0.0370 | 0.1320 | Kirderf  
22 | 39532043 | 0.0990 | 0.0329 | 0.1320 | tamura  
23 | 39417385 | 0.1150 | 0.0166 | 0.1316 | Azzoug Aghiles  
24 | 39528280 | 0.1093 | 0.0217 | 0.1311 | zxf  
25 | 39463924 | 0.1241 | 0.0069 | 0.1310 | ZhiMi4933  
26 | 39528420 | 0.0976 | 0.0320 | 0.1297 | ISAKA Tsuyoshi  
27 | 39530080 | 0.1022 | 0.0273 | 0.1296 | GizMonic Mattstitute  
28 | 39508685 | 0.1167 | 0.0128 | 0.1296 | Nagata  
29 | 39528971 | 0.1257 | 0.0028 | 0.1285 | zzzz  
30 | 39524121 | 0.0851 | 0.0431 | 0.1282 | YOLO  
31 | 39529589 | 0.0976 | 0.0301 | 0.1278 | copasta  
32 | 39025699 | 0.0762 | 0.0498 | 0.1261 | tiod0611  
33 | 39445692 | 0.0939 | 0.0318 | 0.1258 | Max Pham 12  
34 | 39508025 | 0.1006 | 0.0247 | 0.1254 | tingyu516  
35 | 39494089 | 0.0943 | 0.0308 | 0.1251 | Melinda  
36 | 39531201 | 0.0872 | 0.0374 | 0.1247 | ivan  
37 | 39502727 | 0.0944 | 0.0294 | 0.1239 | Feida Wei  
38 | 39529631 | 0.0818 | 0.0419 | 0.1238 | torino  
39 | 39525032 | 0.1037 | 0.0197 | 0.1234 | Dawid Motyka  
40 | 39528121 | 0.0887 | 0.0345 | 0.1232 | unom3  
41 | 39524026 | 0.0988 | 0.0241 | 0.1230 | TuMinhDang  
42 | 39531474 | 0.1103 | 0.0123 | 0.1227 | Joonas Maanonen  
43 | 39447617 | 0.0901 | 0.0326 | 0.1227 | Steve Le (Solokop)  
44 | 39527870 | 0.1074 | 0.0152 | 0.1227 | Farid ????????  
45 | 39421229 | 0.1023 | 0.0201 | 0.1224 | Azzoug Aghiles  
46 | 39531769 | 0.1051 | 0.0171 | 0.1223 | (AI)kinator  
47 | 39529160 | 0.0941 | 0.0279 | 0.1220 | xuling  
48 | 39529389 | 0.0916 | 0.0302 | 0.1219 | Moro  
49 | 39453366 | 0.0729 | 0.0484 | 0.1213 | Bo Peng  
50 | 39495003 | 0.0889 | 0.0323 | 0.1212 | wasjaip  
51 | 39529087 | 0.0985 | 0.0220 | 0.1206 | Krens  
52 | 39531713 | 0.1023 | 0.0178 | 0.1201 | kek  
53 | 39514280 | 0.1086 | 0.0114 | 0.1200 | AlphaBot Assassin  
54 | 39529311 | 0.1043 | 0.0155 | 0.1199 | techniquetwice  
55 | 39528491 | 0.0810 | 0.0388 | 0.1199 | Soyoung Lee  
56 | 39450366 | 0.0802 | 0.0396 | 0.1198 | Vavilkin Alexander  
57 | 39531863 | 0.1019 | 0.0178 | 0.1198 | Vitaly Kudelya  
58 | 39515885 | 0.1283 | -0.008 | 0.1195 | Nagata  
59 | 39530162 | 0.0975 | 0.0210 | 0.1186 | GizMonic Mattstitute  
60 | 39457669 | 0.1137 | 0.0045 | 0.1182 | ZhiMi4933  
61 | 39501169 | 0.0916 | 0.0262 | 0.1179 | ptdtrinh  
62 | 39521063 | 0.1135 | 0.0042 | 0.1178 | Kazuo Watanabe  
63 | 39486867 | 0.0899 | 0.0275 | 0.1175 | Theodore  
64 | 39487411 | 0.1010 | 0.0162 | 0.1173 | Henry Javier  
65 | 39529083 | 0.0937 | 0.0234 | 0.1171 | techniquetwice  
66 | 39528134 | 0.1073 | 0.0097 | 0.1171 | payanotty  
67 | 39516720 | 0.0816 | 0.0352 | 0.1169 | J  
68 | 39531261 | 0.0883 | 0.0282 | 0.1166 | Evgeny Bratkovsky  
69 | 39532168 | 0.0984 | 0.0181 | 0.1165 | Alpha agents are bullying me  
70 | 39530239 | 0.0840 | 0.0323 | 0.1163 | kAIto47802  
71 | 39454425 | 0.0784 | 0.0359 | 0.1143 | Paul Pawletta  
72 | 39506695 | 0.1000 | 0.0142 | 0.1143 | kaoutar  
73 | 39448446 | 0.0867 | 0.0270 | 0.1138 | kzk_t  
74 | 39530796 | 0.0940 | 0.0195 | 0.1136 | Jing Guo  
75 | 39493772 | 0.0809 | 0.0322 | 0.1131 | greg97436  
76 | 39525255 | 0.0879 | 0.0251 | 0.1130 | Ilmari Vahteristo  
77 | 39527982 | 0.0727 | 0.0402 | 0.1129 | Vibhatsu  
78 | 39442501 | 0.0713 | 0.0414 | 0.1128 | -SANTACLAWS-  
79 | 39531908 | 0.1068 | 0.0060 | 0.1128 | (AI)kinator  
80 | 39531582 | 0.0953 | 0.0171 | 0.1125 | Behavior Cloners  
81 | 39524027 | 0.0937 | 0.0186 | 0.1124 | TuMinhDang  
82 | 39451282 | 0.0858 | 0.0264 | 0.1122 | huyphnvn  
83 | 39499595 | 0.0883 | 0.0235 | 0.1119 | dbtmddn41  
84 | 39531681 | 0.0868 | 0.0249 | 0.1118 | SpiralTip  
85 | 39526903 | 0.0819 | 0.0296 | 0.1116 | Wal8800  
86 | 39531492 | 0.0826 | 0.0288 | 0.1114 | KKY  
87 | 39530526 | 0.0908 | 0.0202 | 0.1111 | John Smith  
88 | 39530986 | 0.0913 | 0.0197 | 0.1111 | Joakim Arvidsson  
89 | 39527735 | 0.0932 | 0.0174 | 0.1106 | monkeyHyx  
90 | 39529086 | 0.0864 | 0.0237 | 0.1101 | Krens  
91 | 39505819 | 0.1011 | 0.0088 | 0.1100 | John Smith  
92 | 39383672 | 0.0984 | 0.0113 | 0.1097 | FrederikZ  
93 | 39530250 | 0.0847 | 0.0249 | 0.1097 | BK  
94 | 39484014 | 0.0736 | 0.0355 | 0.1091 | Goya Shuto  
95 | 39440565 | 0.0805 | 0.0286 | 0.1091 | gromml  
96 | 39532072 | 0.0736 | 0.0353 | 0.1089 | filtered  
97 | 39452984 | 0.0799 | 0.0288 | 0.1087 | Yousef Rabi  
98 | 39530244 | 0.0944 | 0.0142 | 0.1086 | gorinars  
99 | 39448439 | 0.0930 | 0.0154 | 0.1085 | hinata1119  
100 | 39532075 | 0.0859 | 0.0225 | 0.1084 | filtered  
  

## 9 Comments


### [Aravind S](/aravind36)
It’s good to see both of our bots in the list— thank you for the analysis.
While we’re ranked 14th on this list, our LB rank hovers around 200. Our bots
were among the unluckiest, losing to alpha bots over 90% of the time.
If only this competition were focused solely on LLMs—how different the results
could have been, looking forward to such competitions in future.


### [gaolicious](/tianzhenggao)
Interestingly, those that neither initialize nor accept the Alpha Agent, like
Pascal Pfeiffer and me, seem to have a better ranking here than on the
leaderboard. I'm surprised to see my bot ranked 6th here.
If only the final leaderboard were calculated like this haha


### [Pascal Pfeiffer](/ilu000)
Haha, thought the same thing. I was hoping for alpha agents to not work on the
private test. But great to see that full LLMs are not too far behind actually


### [Matthew S Farmer](/matthewsfarmer)
The few and the proud pure-LLM agents! Ours is 47th on this list compared to
mid-100's on the leaderboard. Thanks
[@jademonk](https://www.kaggle.com/jademonk) for doing this work! 🫠


### [Mitsutani](/dmitsutani)
This is really great! I'm happy to see that my lb position reflects the list.
Just to be clear, the lift scores here were calculated discarding alpha
episodes? I'm surprised by how low my bot's guesserlift is since just looking
at its episodes I'd say it answers reasonably almost every time EXCEPT with
alpha games (I refused to hard code that into it).
Could there have been some leakage there? Or just variance from getting bad
pairings?


### [Andrew Tratz](/jademonk)
Yes, all alpha episodes are discarded. But there's undoubtedly some bias still
because the match pairings aren't random, and being in a high leaderboard
position increases likelihood of receiving a good partner. My analysis
attempts to reduce this bias but it's still present.


### [VolodymyrBilyachat](/vovikdrg)
I messed up with my submission. Now i regret it. I pushed wrong temperatures.
Now I slightly improved my latest agents which will publish after competition
end…


### [Jonathan Chan](/jonathanchan)
Nice try. Having problem loading the large version 2 output from your notebook
though and cannot check others.
Interesting that 6/10 current top-10 agents are not in the top-50 as LLM
agent.
> To use a sports analogy, it's like computing how much benefit a player
> batting .308 provides to a team compared to another who bats .251.
To use baseball analogy, quite often HR, RBI, and R stats have bigger impact
than the batting average. 😉


### [Andrew Tratz](/jademonk)
I've re-run the notebook from scratch (version 4) and saved the output as a
CSV file -- see if you can view it now.


### [Jonathan Chan](/jonathanchan)
Thanks. It seems my LLM bot is ranked 67th and probably around where it should
be. The latest reset forced it to play 6 straight matches against alpha agents
right after the reset and corrected its standing. 😭
