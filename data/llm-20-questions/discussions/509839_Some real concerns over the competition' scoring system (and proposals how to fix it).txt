[Kha Vo](/khahuras) · 13th in this Competition · Posted 3 months ago


### Some real concerns over the competition' scoring system (and proposals how
to fix it)
Hi Kaggle Team,  
I am truly interested in competing in this competition, but the concerns
raised below also truly made me lose most interest in it. I hope you can
review and make comments or adjustment to the game design, to attract more
people into the contest.  
**1\. There are no true competitiveness in the match-up**  
Unlike all of the previous Kaggle simulation competitions (Halite,
Rock/Paper/Scissors, Snake, ….) where there are real "versus" match, this "20
Questions Game" actually does NOT have any aspects for the "versus" at all. In
specific, how a team (composed of a questioner and an answerer) performs in a
match does NOT have any impact to the opponent's team. As a result, pairing to
form a match-up makes no sense. Indeed, just forming 1 team for a single
"testing" and count the number of questions needed to guess the correct
keyword should be a much better metric on the LB.  
**2\. Tied games (games resulting a draw) are so populous, devastating to the
higher ranked bots, and inexplicably rewarding to the lower ranked bots**  
Since the match-up is not truly competitive at all (explain in point 1), it is
converged to the notion that each bot just needs to care for its own sake, not
caring anything else, not even the opponent's team. However, this is still so
far to guess the correct keyword. The perfect questioner still depends heavily
on the answerer to be able to correctly guess the keyword.  
In short, to actually "WIN" a game is so difficult. It needs to have 3
following aspects to form a win: a) The questioner must be very good in
playing "20 questions". b) The answerer must be at least a valid (and good)
LLM. and c) No error formed by any of the 4 bots  
I saw one of my bots amazingly guessed out the keyword in 4 steps, got 45
points on the LB, jumped to 1st place. Then, in 3 next consecutive games it
acts as the answerer and at least draws all games, but still got heavily
deducted until it cancels out that amazing win.  
The ratio between a good win and the tied games is extremely low. Hence, the
mechanism of point deduction in tied games for higher ranked bots (and also
increasing points for lower bots) need to be rectified, otherwise good bots
can never surface on the top of the LB.  
However, making rectifying on the scoring like that still poses some other
weaknesses that we still can't foresee. I still prefer changing the match-up
format to the "testing" metric, that is: single question-answerer pairing only
with no matchup, and LB is based on the average metric such as:  
0.8* (average number of questions needed to guess correctly as the questioner)
+ 0.2 * (average number of questions needed to guess correctly as the
answerer)  
Please have a look and my points.  
[@bovard](https://www.kaggle.com/bovard)
[@addisonhoward](https://www.kaggle.com/addisonhoward)
[@mylesoneill](https://www.kaggle.com/mylesoneill)


## 8 Comments


### [Bovard Doerschuk-Tiberi](/bovard)
Kaggle Staff

Thanks for your thoughtful post, we are looking into options for getting
better ratings signals. Will update when we have something to share!


### [Bovard Doerschuk-Tiberi](/bovard)
Kaggle Staff

We've adjusted the scoring algorithm to increase the rating gain/loss from
win/loss and reduce the rating gain/loss from ties. This should be live now
but it may take us a bit to dial it in correctly.


### [Bovard Doerschuk-Tiberi](/bovard)
Kaggle Staff

You can see the effects of this new scoring paradigm here:
<https://www.kaggle.com/competitions/llm-20-questions/leaderboard?dialog=episodes-
submission-38522755>
Losing < 5 points for a tie and getting 50+ for a win


### [Kha Vo](/khahuras)
[@bovard](https://www.kaggle.com/bovard) I think losing points in tied games
must also be strictly positive although can be small (a good team must be
deducted at least 1 point in tied game with lower bots). It can't be 0
otherwise you'll have many local minima on the LB, all will come to luck when
a new bot is submitted: who will it be paird to.


### [Kha Vo](/khahuras)
That looks better! Thanks Bovard.


### [tr](/toma78)
I think pairing just 2 agents (both as questioner and answerer) instead of 4
different engines would solve most problems with scoring system and still keep
the original goal of the competition. Probably also a minor change for the
hosts, but the scoring should also change, since there is no more
competitiveness between pairs.


### [JavaZero](/jimmyisme1)
This can lead to malicious misdirection and prevent the adversary from
guessing the correct key word


### [Kha Vo](/khahuras)
[@jimmyisme1](https://www.kaggle.com/jimmyisme1) How?


### [Giba](/titericz)
I posted about it last week but got no reaction at all. LB right now is a
periodic random shuffle. Only hosts can fix the scoring system.  
To be fair, agents should simply alternate between questioner and answerer at
each game. Guessers agent should be given much more points than answerers. A
Draw should give zero (or -1) points to everyone independent of the
level/skill.


### [Kha Vo](/khahuras)
Right Giba. The most annoying part is that a bad answerer will damage the good
guesser so much


### [RS Turley](/rturley)
Many of these problems arise from there being very little differentiation
across rankings in the early leaderboard—so low quality agents are getting
matched in with high quality ones and dragging them down.
Even a small change (like this week’s keyword update?) that creates more
differentiation in LB scores might solve these issues as low quality agents
drop in score and high quality agents match with each other more frequently.
