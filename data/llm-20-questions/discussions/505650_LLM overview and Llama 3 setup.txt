[Raki](/raki21) · 10th in this Competition · Posted 3 months ago


### LLM overview and Llama 3 setup


## Evaluation of Gemma and Other LLMs

### Issues with Gemma
Gemma 7b it quant, which is used in the starting notebook, struggles with
following instructions and doesn't achieve state-of-the-art performance for
its size.

### Recommended LLM Evaluation: LMSYS Arena Leaderboard
To identify a robust general LLM, I recommend the [LMSYS Arena
Leaderboard](https://chat.lmsys.org/). This Elo rating system involves users
asking questions and comparing answers from two different models (blind) to
determine which one they prefer. This evaluation method is challenging to
"game" as success depends on consistently satisfying user queries across
various topics. Other metrics can suffer from benchmark leakage into the
training set and often focus on narrower tasks, making them easier to optimize
for specific performance rather than general utility.

### Current Top Models
  * **GPT-4:** 1287 Elo
  * **Gemini 1.5 Pro (Google):** 1248 Elo
  * **Best Anthropic Model:** 1246 Elo
  * **LLaMA 3 70B Instruct (Meta):** 1203 Elo (best open-source model, but too large)

### Gemma's Performance (like in starter)
  * **Gemma 7B-IT:** 1043 Elo
  * **Quantized Version:** Slightly worse (Quantization reduces weights from formats like FP32 to INT8, significantly lowering VRAM and compute requirements at the cost of some quality)

### Alternative: LLaMA 3 8B Instruct
  * **LLaMA 3 8B Instruct:** 1155 Elo (a much stronger model overall)
We still need to quantize it to run it on the T4 available for submission,
which has 16 GiB of VRAM.

### Best Way to Run Inference: llama.cpp
As far as I know, the best way to run inference on a non-proprietary quantized
LLM is with llama.cpp. This tool employs various techniques to speed up
inference, such as quantization and KV caching.

### Resources for LLaMA 3 8B Instruct
I have previously set up a dataset with some quantized LLaMA 3 8B Instruct
models:
  * [Dataset](https://www.kaggle.com/datasets/raki21/meta-llama-3-8b-gguf)
  * [Notebook](https://www.kaggle.com/code/raki21/llama-3-gguf-with-llama-cpp) demonstrating its use, I'm in the process of adding a Q20 example, with persistence across the chat.
I recommend using the 8-bit quantized variant. Although I don't have the time
to integrate it with the current agent setup, I hope this writeup helps some
people get started!
**AI Note:** I wrote the text myself, but ran it through GPT4o to format it
into a nicer markdown structure and correct typos.


## 1 Comment


### [Melinda](/melindaweathers)
Hi, and thanks for posting this notebook! I am trying to get llama-cpp-python
working in my submission, and if I make a copy of your notebook, I'm able to
run the pip install command in it. However, if I try to pip install it into a
folder with the "-t /kaggle/working/submission/lib" option so that I can
package it up, I get all kinds of dependency resolver errors. ("ERROR: pip's
dependency resolver does not currently take into account all the packages that
are installed. This behaviour is the source of the following dependency
conflicts. (etc - a huge list)")
I'm curious if you have any tips for how to get llama-cpp-python with
llama-3-8b-instruct packaged for a submission?
