[loh-maa](/lohmaa) ¬∑ 3rd in this Competition ¬∑ Posted a month ago


### Alpha notebook, maybe the final one [LB 666+]
In the spirit of Conan the Barbarian and his majesty Mad Max the second! I
hereby share the partial solution based on [alphabetical
search](https://www.kaggle.com/code/lohmaa/llm20-agent-alpha). I spent time to
make it so simple and elegant, that I am even delighted. It is not a complete
solution, and that's actually splendid because it leaves room for adaptation
and improvement, especially regarding the way to finish a failed alpha search.
I know some people dislike this whole idea though, perhaps because it's not
based on LLMs, maybe for other reasons, but for me that's a bit irrational.
Why?
  1. First of all, it is not against the rules, and it is not unethical. Perhaps it was not entirely expected by the concept of this competition, but unexpected approaches are not a bad thing per se.
  2. It is optimal when the keyword space is publicly known and the answerer can answer it.
  3. Yet, contrary to some opinions, even if the keyword space is not known, it's not at all useless..
     * for one, simply because one can still have many keywords on the list covered,
     * for two, because one can combine it with LLMs.
  4. There are many other solutions which basically do a similar thing, except less efficiently, e.g. asking questions about first letters, or whether the keyword is "on the following list". Even the ex-top solution so widely adored is based on such techniques. Surely there are some pros to those, too, I will not elaborate here.
Finally, as a matter of adoption -- we may say Alpha is not reliable because
many agents accept the handshake and then answer the Alpha questions
incorrectly, and that's a valid point. However, regardless of whether such
behavior is intentional or unintentional, _it is never in the legitimate
interest of an agent to fail the team on purpose_. So if anybody still doesn't
like or believe that lexicographical search is effective, they can simply
refuse the handshake and the team may try its luck in another way. In an
interesting way it's a clash between rational vs irrational.


## 4 Comments


### [KKY](/evilpsycho42)
This dicussion/notebook deserves more votes. Although all of us hope the LLM
would the key to this competition, the reality is Agent Alpha is the key.
[@lohmaa](https://www.kaggle.com/lohmaa) reported that Agent Alpha got 26%
success rate while other agents got 2.2% sucess rate, huge difference.
I belive the most important reason for the poor performance of the LLMs is the
limitation of computing resouces (FLOP/VRAM/Time).  
We can't use more intelligent models, and even we have to quantize the full 8B
model, make them even worse ‚Ä¶ Also duto the limitation of excuting time, we
can do a lot of cot / react / multiple agent and so on.
But also we can see some team on the golden zone who don't use AgentAlpha,
looking forward to he furture matches.


### [OminousDude](/max1mum)
Very good code üëçüëçüëçüëçüëçüëçüëç


### [loh-maa](/lohmaa)
[@max1mum](https://www.kaggle.com/max1mum) Thanks, well technically it's not a
model, but anyway, from downvotes it seems some people disagree with your
opinion.. I'd love to hear what is it exactly that they dislike or disapprove
about it. So if anybody knows, then please help me understand.


### [Sahir Maharaj](/sahirmaharajj)
Thanks for sharing, [@lohmaa](https://www.kaggle.com/lohmaa)
