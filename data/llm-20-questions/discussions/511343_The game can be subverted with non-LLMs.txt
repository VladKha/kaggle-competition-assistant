[loh-maa](/lohmaa) · 3rd in this Competition · Posted 3 months ago


### The game can be subverted with non-LLMs
It's just a hypothesis for now, but I believe a near-optimal solution to the
game doesn't involve LLMs. It's possible to ask questions using plain regular
expressions, and if the answering agent is able to understand, the answer will
always be perfect -- a huge advantage over LLMs. Also bisecting the space is
much more efficient with REs than with natural language.
The only thing missing is.. adoption. Once there is a critical mass, REs
should be able to take over the top, or at least be necessary to stay on top.
Very interesting dynamics regarding who's gonna talk which language.. and of
course the critical question -- are we going to see any new regulation
regarding this?


## 7 Comments


### [loh-maa](/lohmaa)
Update
Let me share some thoughts on the situation, please.
  1. In the current phase, when the list of keywords is known, **optimal game plays exist** , and they're not based on LLMs. The only factor holding them back from domination is lack of adoption or a common protocol to apply them during the games. They can be based on many techniques, I thought of regular expressions first but soon I realized a simple alphabetical bisection is just perfect. I published a [notebook here.](https://www.kaggle.com/code/lohmaa/llm20-agent-alpha)
  2. In the evaluation phase, when the list of keywords is unknown, the performance of those previously optimal solutions would depend on the assumed list of keywords, but even with rough assumptions I think it's going to be superior to LLMs. Perhaps we may call such solutions near-optimal. Here I must wave to our dear hosts [@bovard](https://www.kaggle.com/bovard) [@addisonhoward](https://www.kaggle.com/addisonhoward) \-- I'm afraid changing the list of keywords will not suddenly favor LLMs, and thus it won't solve this issue.
  3. Suppose all players play the "LLM" game by default, but there is a group of people that (secretly or openly) agreed to apply the optimal strategy whenever they are paired in a team. Surely such a group would come on top of the LB, given everything else being equal. And surely nobody would like to be outplayed by a group of friends who secretly adopted an optimal protocol. If it was done openly and not last minute, then perhaps it would be fair, but let's not hope for the best, we must assume collusion attempts will be made. So what can be done?
  4. With the current format of the game, one thing that decent players can do is to adopt a common protocol to apply the optimal play (spontaneously.) This would effectively eliminate any advantage of secret collusion (aka rigging?) However this would also shift the competition from LLMs to a very simple conventional solution, which is fine to me and good for the planet, but this would miss the goal of the competition.
  5. Perhaps there is one path to keep the LLMs in play, though, if only the agents would have to play with a wide spectrum of players in the ranking, and provided some players (or perhaps "neutral" agents?) would play LLM only -- this would force everybody to be at least able to play LLMs as well. (Then however, is another question whether the ELO/ranking can work in such a mode… BTW if it was published we could do simulations.)
  6. Another option would be to change the format and instead of teaming players force the play against a neutral/reference LLM which would not adhere to any particular protocols except natural language. This in turn would make the competition all about trying to figure out the reference LLM -- what it can understand and why not.
  7. I don't think the above remedies are great, but still sound better to me than falling to collusion. I am very interested in your opinion.


### [tr](/toma78)
I agree with you. My conclusion was that the list of viable keywords has to be
fairly limited (<50K) anyway, so not really unknown, so solvable with
"classical" approaches, at least for questioner/guesser. For the answerer I'm
still not sure since it is more dependent on "protocol" of other agent, but
looks viable.
EDIT: maybe the list of viable keywords is bigger than my estimate, wikipedia
has almost 7M articles.


### [loh-maa](/lohmaa)
Thanks for sharing your perspective, well, if I understand correctly, non-LLM
approaches are not more restricted than LLMs.. how many keywords an LLM of
size 7b can handle, practically? Maybe 1 or 2k? So far it's not looking very
effective even for 560.


### [tr](/toma78)
Sorry for confusing posts, maybe I'm just rambling, but I'll try to elaborate
:)
Yes, I think classical approach is valid at least as LLM approach, but only
for questioner/guesser.
Classical would need a list of keywords to perform some kind of bisection with
hard-coded questions and guesses from the list. I believe such list can be
made, since I estimate <50K viable places, persons or things. Such agent would
be optimal, depending on accuracy of answerer and completeness of the list
(like in your notebook).
LLM questioner/guesser doesn't strictly need such list, but I expect it to be
inferior to classical approach above. Thus defeating part of the goal of
competition.
For LLM "word capacity", in theory I'd estimate much higher. I mean, you can
ask it pretty much about any entity and it will give you a reasonable reply.
No?


### [loh-maa](/lohmaa)
Yes, well, non-LLM works only when paired with another compatible non-LLM, and
then they make one solution.
Yes, the "word capacity" is probably much higher than my estimate, but then
what is the power of questions and reliability of answers as the search
descends into more details? Even a small error rate in answering can affect
the search, 20% error rate usually ends in a bush.. certainly improving this,
is the primary objective behind this competition, and optimistically, as long
as non-LLMs need LLMs for a backup it may prevail.


### [Kha Vo](/khahuras)
[@bovard](https://www.kaggle.com/bovard)
[@addisonhoward](https://www.kaggle.com/addisonhoward)  
As shown by this topic's author, his agent is now starting to dominate the
leaderboard. More teams are submitting this agent which is a binary search
based solely on the keyword list. Both questioner and answerer of this agent
are non-LLM, just rule-based. So as long as many more teams submit this agent,
they will all occupy top of LB very soon.
You can see one episode example here:
<https://www.kaggle.com/competitions/llm-20-questions/submissions?dialog=episodes-
episode-55060055>
I guess all these below changes may have to be made at the same time to make
the competition better:
  * List of keywords must be changed
  * List of keywords must not be accessible by any agents by any means, even from this early stage
  * Replays must not display the keyword publicly. Replays may not also display guesses.
  * Keywords must not be obtained via aggregating past gameplays


### [loh-maa](/lohmaa)
Hello Kha Vo, I share your concern about the situation, it's not going in the
right direction, however I think the remedy you just proposed is not a real
solution. Non-LLM optimal solution would become conditionally optimal (if
that's the right term) but I bet it would stay in play even after those
changes. Perhaps they would make adoption slower, but they would also make the
current testing stage unclear and less fun.
As I stated in the update, I think that common adoption of a near-optimal
solution based on an open protocol is perhaps the only effective protection
against collusion in the evaluation phase, given the current game format. If
you can think of any other way to neutralize the advantage of potential
collusion (again, given the current game format), then please show me wrong.


### [Max Brown](/maxbr0wn)
I don't see how your proposed changes would work. The bisecters can just
create their own set of keywords (up to hundreds of thousands of words) and
distribute this common set amongst themselves.


### [Krens](/jickymen)
Using non-LLM methods such as binary search is indeed a different and even
brilliant solution. The difficulty of this method should be how to build a
list that "completely" covers all possible keywords. Theoretically, we can
search for millions of keywords in one round of competition, but when the
search fails, we can also consider adding LLM to remedy it. (Although the
effect should not be too good)


### [Max Brown](/maxbr0wn)
I'm only just looking at this competition now. A binary search based solution
was the first thing that occurred to me as well. Is there any indication that
the kaggle team is going to address this?
I'm struggling to think of how to fix this.
