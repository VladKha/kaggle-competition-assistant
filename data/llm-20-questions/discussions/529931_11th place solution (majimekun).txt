[NIWATORI](/niwatori) ¬∑ 11th in this Competition ¬∑ Posted 9 days ago


### 11th place solution (majimekun)
This is majimekun team‚Äôs solution. majimekun is composed of
[@niwatori](https://www.kaggle.com/niwatori),
[@akimaru](https://www.kaggle.com/akimaru), and
[@keisho](https://www.kaggle.com/keisho).
This competition was uniquely interesting. I would like to express my
gratitude to the organizers. However, we believe the competition might have
been even more engaging without the binary search component, allowing for more
diverse strategies and creative problem solving.
# Summary
Our solution is primarily classified as an offline policy approach. The main
motivation behind adopting the offline policy is that while an LLM with 7~9B
parameters may struggle to make accurate guesses based on 20 question-answer
pairs, it performs well when answering a single keyword and question pair.
We prepared a keyword list, a question list, a probability table for all the
(keyword, question) pairs, and prior probabilities of keywords beforehand.
During inference, we derive an optimal question by minimizing conditional
entropy, and guess a keyword with maximum likelihood. Our keyword list was
generated using LLMs, our question list was obtained from public replays and
LLMs, and the probability table was calculated from LLMs.
According to [the nice
statistics](https://www.kaggle.com/competitions/llm-20-questions/discussion/529683)
by [@lohmaa](https://www.kaggle.com/lohmaa), the win ratios of our pure LLM
Guessers during the evaluation period are (it should be noted that the win
probability depends on paired answerers)
  * submission_id=39526145 (K=10): 3/14 (21.4%)
  * submission_id=39527069 (K=400): 5/40 (12.5%)
, where K is a parameter that limits the ratio between maximum and minimum
prior probabilities of keywords.
# Answerer
The Answerer component is relatively straightforward. It consists of a few
rule-based logics and majority voting among three LLMs.


## Rulebases
The Answerer responds if the question matches the following regular
expressions:
  * startswith and endswith: `(starts|start|begins|begin|end|ends) with the letter [\'"]?([a-zA-Z])[\'"]?\?$`
  * contains: `(includes|include|contains|contain) the letter [\'"]?([a-zA-Z])[\'"]?\?$`
  * dictionary order: `keyword.*(?:come before|precede) \"([^\"]+)\" .+ order\?$`
The dictionary order regex captures the Agent Alpha type questions as well.


## LLMs
If the question cannot be answered by the rulebases, we rely on majority
voting of three 4-bit quantized LLMs: gemma-9B, llama3.1-8B, and mistral-7B.
We assumed that majority voting would be more robust than using a single 8-bit
model in such a noisy environment, so we opted to use the three models.
Initially, four models, including phi-3-small, were considered, but after a
rough simulation, we selected the final three models.
Since loading more than two models simultaneously exceeds the GPU memory
limit, we load and unload models for each inference. We reduced the number of
model loads per turn from three to two by rotating the inference order like
ABC/CAB/BCA/‚Ä¶, where A, B, and C represent each model.
Initially, we used the probabilities of model outputs obtained by summing up
contributions from representing tokens. However, this method performed worse
than the simple majority voting based on text generation. It‚Äôs possible that
some tokens were missing in the probability calculation in our implementation.
# Guesser
Our Guesser is based on the offline policy. We prepared a keyword list, a
question list, a probability table for keyword and question pairs, and prior
probabilities of each keyword.
Deriving the optimal question and the optimal keyword follows the
straightforward logic. Given a list of known question-answer pairs, the
optimal question is determined by minimizing the conditional entropy:
ùëéùëüùëîùëöùëñùëõùëû‚àëùëß=ùëåùëíùë†,ùëÅùëúùëù(ùëû=ùëß|ùëû1,‚Ä¶,ùëûùë°)[‚àí‚àëùëòùëù(ùëò|ùëû1,‚Ä¶,ùëûùë°,ùëû=ùëß)ùëôùëúùëîùëù(ùëò|ùëû1,‚Ä¶,ùëûùë°,ùëû=ùëß)],argminq‚àëz=Yes,Nop(q=z|q1,‚Ä¶,qt)[‚àí‚àëkp(k|q1,‚Ä¶,qt,q=z)logp(k|q1,‚Ä¶,qt,q=z)],
where ùëòk represents a keyword, ùëûq represents a question, and ùëû1,‚Ä¶,ùëûùë°q1,‚Ä¶,qt
are the already asked questions (and answers). This is calculated using Bayes
theorem, complexity of the numerical calculation is O(KQ), where K is the
number of keywords and Q is the number of questions.
The optimal keyword can be derived by
ùëéùëüùëîùëöùëéùë•ùëòùëù(ùëò|ùëû1,‚Ä¶,ùëûùë°).argmaxkp(k|q1,‚Ä¶,qt).
We also incorporated the prior probabilities for the keyword ùëù(ùëò)p(k). The
logic of deriving the prior probabilities is described in the next section.


## Keyword list generation
A keyword list is crucial in an offline policy since we cannot guess a keyword
that isn‚Äôt in our list. However, the more keywords we have, the harder it is
to find the right answer. Poor keywords can negatively affect both question
selection and actual guesses. Therefore, we have to find a good balance of
recall and precision to some extent.
To evaluate the quality of a keyword list, we randomly separated the public
keywords into a training keyword list and a validation keyword list. We
generated a keyword list using only the training keywords, and evaluated
recall and precision (or num of vocab) with the validation list.
We experimented with various methods to generate the keyword list:
  * Using all descendants of `physical_entity.n.01` in the WordNet
    * Recal: ~46%, Vocab size: ~35000
  * Using neighbors of training keywords in terms of WordNet (e.g., children of parents of each keyword)
Ultimately, we found that generating keywords with LLMs (ChatGPT and Gemini)
produced surprisingly better quality. We hypothesize that the Kaggle host also
used LLMs to generate their keyword list. By providing a subset of training
keywords as few-shot examples, we collected keywords by repeating the process
multiple times.
Another advantage of using LLMs is that we can estimate the likelihood of
appearance in the public/private keyword list based on how often the keyword
is generated by the LLMs. We found a clear correlation between precision and
frequency. Leveraging this correlation, we defined prior probabilities based
on these frequencies, and included them into the optimal question and guess
calculations. Thanks to the prior probabilities, we can effectively improve
precision while maintaining the number of vocabulary.
The figure below shows the number of keywords and recall for the validation
keyword list, as the frequency threshold changes. The right most points
correspond to frequency>=2. After a rough local simulation, we choose 2 as the
threshold.
![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-
attachments/o/inbox%2F661313%2Fdb9b6cff8756b07b14005686174b7901%2F.png?generation=1724425661059538&alt=media)
After removing clearly non-object keywords with the help of LLMs, our final
keyword list contained approximately 32,000 entries.


## Question list generation
Since the optimal question is automatically derived, having more questions
improves the Guesser‚Äôs performance. The bottleneck was the computational
resource to calculate the p(keyword, question) probabilities table.
We prepared questions from:
  * Rulebases such as ‚ÄúDoes the word start with ‚Äòa‚Äô?‚Äô
  * Questions collected from the public episodes
  * Questions generated by LLMs
The final question list contained approximately 1,000 questions.
One regret is that we should have added more questions. According to the
replays, our bots sometimes end up asking many ‚Äústartswith‚Äù questions,
possibly because the questions are too coarse and there are no more useful
questions.


## Probability table labeling
We need p(keyword, question), which is the probability of ‚Äúyes‚Äù answer for
each keyword and question pair.  
We assumed that many Answerers in the environment use LLMs, and on average,
p(keyword, question) calculated locally with LLMs would not differ
significantly from the actual environment. Therefore, we used the average
probabilities of four LLMs: gemma-9B, llama3.1-8B, mistral-7B, and
phi-3-small. We derived the ‚Äúyes‚Äù probabilities by summing up token
probabilities for ‚Äúyes‚Äù, ‚Äúright‚Äù, ‚Äúcorrect‚Äù and similar terms.  
We used vLLM to speed up the inference for all models except for gemma-9B
(because of OOM or some reasons). This process required decent amount of
computational resources.
# Agent Alpha
We include the Agent Alpha capability in both of 2 bots. The vocabulary is a
simple combination of words from WordNet and the keyword list used in the
Guesser. The vocabulary size is approximately 173,000.
One regret is that since we could predict that there would be many bots with
Agent Alpha capability, and the number of steps to find answers strongly
affects the win rate in matches between binary search bots, we should have
improved the Agent Alpha part more carefully. This could have included
choosing the keyword list more thoughtfully, or prioritizie likely words based
on the prior probabilities as discussed above.


## 8 Comments


### [hoon0303](/hoon0303)
excellent solution!! Congratulations on winning the gold medal!


### [Qihang Wang](/wangqihanginthesky)
Great solution! Congratulations on your promotion to GM.


### [Andrew Tratz](/jademonk)
Thank you very much for sharing. I think it was very clever to calculate prior
probabilities for the keywords and also to use the token probabilities in
computing your information gain. Very elegant! I learned a lot from reading
this. Best of luck in the competition.


### [NIWATORI](/niwatori)
Thank you! I'm glad you found the approach useful. Best of luck to you as well
in the competition!


### [JK-Piece](/jeannkouagou)
Great solution writeup. Congratulations for being in the gold zone. It seems
you have combined several strategies. I wonder which one contributes most to
the overall pipeline


### [NIWATORI](/niwatori)
Thank you! Although it's difficulnt to evaluate quantitatively, according to
experiences, generating keywords using LLMs plays an important role in our
pipeline, and using average scores from multiple LLMs seems effective for
improving robustness.


### [ISAKA Tsuyoshi](/isakatsuyoshi)
Thank you for sharing such an amazing solution! I have seen your team climb
the public leaderboard multiple times with a very high win rate, and your
robust solution on the private leaderboard is truly commendable.


### [NIWATORI](/niwatori)
Thank you! I'm also impressed with your write up.


### [ISAKA Tsuyoshi](/isakatsuyoshi)
Congratulations on winning the gold medal! Thank you for sharing such a clear
and understandable solution!  
In my solution, instead of using the "Things-likeness" as the prior
probability of a keyword, I envisioned the answers to each question as the
basis vectors for the keywords. Therefore, I limited the number of questions
to 521 and developed an algorithm that prioritized extracting keywords that
matched well with those questions. As a result, my agents rarely chose rule-
based questions in the latter round.  
However, after reading your team's solution and the 1st place solution, I
regret not increasing the number of questions as well üòÇ.
