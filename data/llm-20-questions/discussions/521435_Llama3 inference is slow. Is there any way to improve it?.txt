[yamitomo](/yamitomo) · 143rd in this Competition · Posted a month ago

### Llama3 inference is slow. Is there any way to improve it?
It takes more than a minute to generate a single response in Llama3. Is there
any way to speed it up?


## 4 Comments


### [torino](/pnmanh2123)
you can use 8 bits or 4 bits quant, i use 4bits then it need about 4-6 minutes
in simulation all 20 round on 1 t4 gpu.


### [yamitomo](/yamitomo)
Thank you, I will try that.


### [Matthew S Farmer](/matthewsfarmer)
Not my experience… Mind posting your code for initiating the model? Are you
generating thousands of tokens? Do you have the end token and proper pipeline
or chat template loaded?


### [yamitomo](/yamitomo)
The initialization code is as follows:
    
    
    torch.backends.cuda.enable_mem_efficient_sdp(False)
    torch.backends.cuda.enable_flash_sdp(False)
    
    model_id = "/kaggle/input/llama-3/transformers/8b-chat-hf/1"
    
    if debug:
        llm_model = None
    else:
        llm_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")
    
    questioner_agent = None
    answerer_agent = None
    guesser_agent = None
    
    def initialize_agent(obs):
        global questioner_agent, answerer_agent, guesser_agent
        global llm_model
    
        match obs.turnType:
            case "ask":
                questioner_agent = Questioner(llm_model, debug)
            case "answer":
                answerer_agent = Answerer(llm_model, debug)
            case "guess":
                guesser_agent = Guesser(llm_model, debug)
    
    def my_agent_fn(obs, cfg):
        match obs.turnType:
            case "ask":
                if questioner_agent is None:
                    initialize_agent(obs)
                return questioner_agent.get_question(obs)
            case "answer":
                if answerer_agent is None:
                    initialize_agent(obs)
                return answerer_agent.get_answer(obs)
            case "guess":
                if guesser_agent is None:
                    initialize_agent(obs)
                return guesser_agent.get_guess(obs)
    
    
    content_copy
The output token generation code is as follows.
    
    
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from logging import getLogger
    
    logger = getLogger(__name__)
    
    def get_formatted_prompt(prompt, desc=None):
        prefix = "| "
        modified_prompt = "\n".join(prefix + line for line in prompt.split("\n"))
    
        formatted_prompt = ""
        if desc is None:
            formatted_prompt += ("-" * 30) + "\n"
        else:
            formatted_prompt += ("-" * 15) + f" {desc} " + ("-" * 15) + "\n"
        formatted_prompt += modified_prompt + "\n"
        formatted_prompt += "-" * 30
    
        return formatted_prompt
    
    
    class Questioner:
        def __init__(self, llm_model, debug=False) -> None:
            print("Initializing model (Questioner 004)")
    
            self.debug = debug
    
            # 提出時変更必要！！！！！！！
            model_id = "/kaggle/input/llama-3/transformers/8b-chat-hf/1"
    
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = llm_model
            self.id_eot = self.tokenizer.convert_tokens_to_ids(["<|eot_id|>"])[0]
    
        def get_question(self, obs):
            sys_prompt = """You are a helpful AI assistant, and your are very smart in playing 20 questions game,
            the user is going to think of a word, it can be only one of the following 3 categories:
            1. a place
            2. a person
            3. a thing
            So focus your area of search on these options. and give smart questions that narrows down the search space\n"""
    
            ask_prompt = sys_prompt + """your role is to find the word by asking him up to 20 questions, your questions to be valid must have only a 'yes' or 'no' answer.
            to help you, here's an example of how it should work assuming that the keyword is Morocco:
            examle:
            <you: is it a place?
            user: yes
            you: is it in europe?
            user: no
            you: is it in africa?
            user: yes
            you: do most people living there have dark skin?
            user: no
            user: is it a country name starting by m ?
            you: yes
            you: is it Morocco?
            user: yes.>
    
            the user has chosen the word, ask your first question!
            please be short and not verbose, give only one question, no extra word!"""
    
            chat_template = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{ask_prompt}<|eot_id|>"""
    
            chat_template += "<|start_header_id|>assistant<|end_header_id|>\n\n"
    
            if len(obs.questions) >= 1:
                for q, a in zip(obs.questions, obs.answers):
                    chat_template += f"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n"
                    chat_template += f"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    
            question = self._call_llm(chat_template)
    
            return question
    
        def _call_llm(self, prompt):
            # print_prompt(prompt=prompt, desc="prompt to generate QUESTION")
            logger.debug("\n\n" + get_formatted_prompt(prompt=prompt, desc="prompt to generate QUESTION"))
    
            if self.debug:
                return "Is it a bug?"
    
            inp_ids = self.tokenizer(prompt, return_tensors="pt").to("cuda")
    
            # max_new_tokens必要か？？？？
            out_ids = self.model.generate(**inp_ids, max_new_tokens=15).squeeze()
    
            start_gen = inp_ids.input_ids.shape[1]
            out_ids = out_ids[start_gen:]
    
            if self.id_eot in out_ids:
                stop = out_ids.tolist().index(self.id_eot)
                out = self.tokenizer.decode(out_ids[:stop])
            else:
                out = self.tokenizer.decode(out_ids)
    
            return out
    
    
    content_copy


### [yamitomo](/yamitomo)
I get output like this, is it relevant?
    
    
    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
    
    
    content_copy


### [torino](/pnmanh2123)
[@yamitomo](https://www.kaggle.com/yamitomo) it not error, don't relevant to
performance, you can ignore it. I see you was create llm_model inside class, i
don't make sure but may be it is reason you code slow, you can try create
model one time in outside class and use it for all questioner, answer. Slow
can come from kaggle_environment, we don't know how it work.


### [yamitomo](/yamitomo)
llm_model is not created within a class, it is created in the global scope.  
Are you talking about the tokenizer?


### [Matthew S Farmer](/matthewsfarmer)
Device type set to auto… Did you ensure that the model was loaded into GPU
memory? You can check the session starts in the notebook. If not, explicitly
call 'cuda'


### [yamitomo](/yamitomo)
When I changed device_map to cuda and ran it as shown below, a new error
occurred.  
This shows that the model was not completely loaded into the GPU, but I don't
know how to work around this new error.
    
    
    llm_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="cuda")
    
    
    content_copy
ERROR:
    
    
    OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 
    
    
    content_copy


### [SpiralTip](/spiraltip)
I think you need to set up quantization. It might work if you try the
following:
    
    
    bnb_config = BitsAndBytesConfig(
           load_in_4bit=True,
           bnb_4bit_quant_type='nf4',
           bnb_4bit_compute_dtype=torch.bfloat16,
           bnb_4bit_use_double_quant=True,
       )
    
    
    content_copy
    
    
    llm_model = 
    AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="cuda", quantization_config=bnb_config )
    
    
    content_copy


### [yamitomo](/yamitomo)
Thanks to everyone's advice, we are now able to perform inference much faster
than before, although the accuracy of the output has not yet been verified.  
I think this will be very useful for future work.
