[LH](/leonhardhochfilzer) Â· 3rd in this Competition Â· Posted 20 days ago


### 3rd Place Solution | custom stable CV & a missed chance for 1st place
This is the third place solution of "DerTeam" that
[@jonasgoe](https://www.kaggle.com/jonasgoe) and I submitted for the Rohlik
Orders Forecasting Challenge.
First of all I would we would like to thank
[@mkecera](https://www.kaggle.com/mkecera) and Rohlik for hosting this
competition! Ever since I signed up on Kaggle I've been looking out to find
time series forecasting competitions and this one is the first one I could
participate in - this makes it even more exciting for us that we were able to
score such a good result.
**Final result** : 3rd place. Public LB score: `0.0295`, private score:
`0.0371`. You can find a cleaned up version of our final submission
[here](https://www.kaggle.com/code/leonhardhochfilzer/3rd-place-solution-
rohlik-orders-forecasting)
# Final submission


## Model
The model we ended up using was a voting regressor made up of two lightgbm
models and two sklearn histogram gradient boosting regressors with different
parameters. We found this to be a very stable ensemble and parameter tweaks
rarely changed the public Leaderboard score or our CV score in a significant
manner. Previously we used a single LGBM model, which we found much more
sensitive to small changes in the parameters or features.


## Features
We used all features that could be extracted from the data provided by the
organisers. In particular we used the information from the calendar files.
This, in combination with adding lagged features for the holidays and school
holidays lead to improvements from what we could observe. We also added Easter
Sunday, since that seems to not have been present in any of the calendar data
for some reason. Finally we added the usual features such as day of the week,
week of the year etc, as well as the sine of these (adding the cosine _and_
the sine of these time based features led to worse results from our end).

### A missed chance
Unfortunately one feature we did not end up using: A variable indicating the
country in which the warehouse was located. This would have led to a private
score of `0.0355` (as you can verify in the notebook). We tried adding this
feature at some point and it led to a slightly worse public leaderboard score,
and it didn't seem to make a big difference in CV with the model that we were
using at the time, hence we omitted this feature fairly early
(unfortunately!).


## Cross-validation
Since this is a time series forecasting problem, it is inherently problematic
to use a KFold Cross validation because of obvious data leakage problems.
Instead we decided to use a time series split. Initially we observed that a
cross validation score from a time series split didn't quite correlate too
well with the public leaderboard score. This was probably one the most crucial
aspects of the competition. We ended up using two approaches, which seemed to
give us more reliable results:
  1. Use a normal time series split, but exclude "unstable" splits in which the training data for a specific warehouse is 'too small' (e.g. a threshold of at least 100 training examples per warehouse).
  2. Use a custom time series split: 
  * 1st Split: Train: - 15.03.23, Validation: 16.03.23-15.05.23
  * 2nd Split: Train: - 15.02.24, Validation: 16.02.24-15.03.24
The paradigm shift for the Munich warehouse also meant that if our new CV was
fairly good, due to the observed stability of our model we would prioritise a
model with a better public LB score - looking back at our submissions and how
they performed on the private set this strategy seems to have fared pretty
well.
# Approaches that didn't really work for us
  * Using lagged features for orders.
  * Predict user_activity_2 through various different methods.
  * Use features (such as `trend`) generated by Prophet to predict `user_activity_2` or predict the orders.
  * Use a seperate model for Munich or train the other warehouses without Munich. Or any variation of this idea.
> Since Munich is an outlier in the data we thought we could get an edge if we
> improved the score here. Some attempts improved the CV or the validation
> score on the last split but the public LB score was drastically worse.
  * Training with the logarithm of the orders.
> This is in contrast with the 2nd place winners, who reported gains with this
> method.
  * Using MAE to train our model gave terrible results for some reason.
> This is in stark contrast with the 2nd place winners, who reported gains
> with this method.
  * `XGBoost` or `CatBoost` both didn't work very well for us.
  * Like the 2nd ranked team we tried to change the importance of the holidays but we couldn't really detect any improvement doing this.
# Key takeaways for us
Being able to develop a reliable CV score I think was key for success for this
competition. Since it took use some time to find out a good way of dealing
with this (which even in the end was far from perfect) meant that it was hard
to compare previous approaches and techniques with our final result without
rerunning all of them.


## 5 Comments


### [Tanishk Patil](/tanishkpatil)
Great notebook for Reference.  
I really appreciate your work


### [MichalKecera](/mkecera)
Hi [@leonhardhochfilzer](https://www.kaggle.com/leonhardhochfilzer) and
[@jonasgoe](https://www.kaggle.com/jonasgoe), can you pls reach out to me with
the contact user functionality so that we can agree next steps re prizes. Thx.


### [JingQi Yan](/hexinrong)
Great notebook.I really appreciate your work


### [Ogwal Odyek](/ogwalakello)
Again congrats to you and your teammate. You said, **"Ever since I signed up
on Kaggle I've been looking out to find time series forecasting
competitions.."**.  
Does that mean you are a domain expert in times eries forecasting?  
By the way, i like the thought process in your notebook, the code is neat tooðŸ˜¤


### [techniquetwice](/techniquetwice)
Great notebook. Congratulations to your team for reaching the top 3.
I also tried use country as column, but I was pivot it to sparse matrix then
it don't work well. The reason maybe I was pivot warehouse column, it's
similar with country columns.
